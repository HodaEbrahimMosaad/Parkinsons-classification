{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# organize imports\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# seed for reproducing same results\nseed = 9\nnp.random.seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load pima indians dataset\ndf = pd.read_csv('/kaggle/input/parkinsons-data-set/parkinsons.data')\ndata = df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def draw_missing_data_table(df):\n    total = df.isnull().sum().sort_values(ascending=False)\n    percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    return missing_data\n\ndraw_missing_data_table(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors = data.drop(['name'], axis = 1)\n\npredictors = predictors.drop(['status'], axis = 1)\nX = predictors\nY = data['status']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from imblearn.under_sampling import RandomUnderSampler\n# rus = RandomUnderSampler(random_state=0, replacement=True)\n# X_resampled, y_resampled = rus.fit_resample(X,Y)\n# len(X_resampled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = .25, random_state = 7)\n\nfrom sklearn.preprocessing import StandardScaler  \nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data, hue='status')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.hist(figsize=(28,28))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.hist(bins=50, figsize=(28,28))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,15))\nsns.heatmap(data.corr() ,annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.plot(kind='box', subplots=True, layout=(5,5), sharex=False, sharey=False, figsize=(28, 28))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = data\nall_cols = list(dataset.columns.values)\nall_cols.remove('name')\nall_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = np.linspace(-10, 10, 30)\nplt.figure(figsize=(15,30))\nfor i in range(1, 22):\n    plt.subplot(14, 2, i)\n    col = all_cols[i]\n    plt.title(all_cols[i])\n    plt.hist(data[col][data.status == 1], alpha=0.5, label='x')\n    plt.hist(data[col][data.status == 0], alpha=0.5, label='y')\n    \n\nplt.legend(loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier()\nmodel.fit(X_train, Y_train)\ny_pred = model.predict(X_test)\n# summarize the fit of the model\nprint(\"KNeighborsClassifier: \")\nprint(metrics.accuracy_score(Y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nmlp = MLPClassifier(hidden_layer_sizes=(22,18,12),max_iter=1500)\nmlp.fit(X_train,Y_train)\ny_pred = mlp.predict(X_test)\nprint(\"MLPClassifier: \")\nprint(metrics.accuracy_score(Y_test, y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nmodel = XGBClassifier()\nmodel.fit(X_train, Y_train)\ny_pred = model.predict(X_test)\nprint(\"XGBClassifier: \")\nprint(metrics.accuracy_score(Y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = Sequential()\n#First Hidden Layer\nclassifier.add(Dense(16, activation='relu', kernel_initializer='random_normal', input_dim=22))\n#Second  Hidden Layer\nclassifier.add(Dense(8, activation='relu', kernel_initializer='random_normal'))\n#Output Layer\nclassifier.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))\n#Compiling the neural network\nclassifier.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])\n#Fitting the data to the training dataset\nclassifier.fit(X_train,Y_train, batch_size=10, epochs=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate the model\ntscores = classifier.evaluate(X_test, Y_test)\nprint(\"Test Accuracy: %.2f%%\" %(tscores[1]*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trscores=classifier.evaluate(X_train, Y_train)\nprint(\"Train Accuracy: %.2f%%\" %(trscores[1]*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclassifier.save_weights(\"model0.h5\")\nprint(\"Saved model to disk\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_json = classifier.to_json()\nwith open(\"model0.json\", \"w\") as json_file:\n    json_file.write(model_json)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import model_from_json\n\njson_file = open('./model0.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n# load weights into new model\nloaded_model.load_weights(\"./model0.h5\")\nprint(\"Loaded model from disk\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate loaded model on test data\nloaded_model.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])\nscore = loaded_model.evaluate(X_test, Y_test, verbose=0)\nprint(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Try different machine learning models and algorithms","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom xgboost import Booster\n\nmodel._Booster.save_model('model.bin')\n\ndef load_xgb_model():\n    _m = XGBClassifier()\n    _b = Booster()\n    _b.load_model('model.bin')\n    _m._Booster = _b\n    return _m\n\nmodel = load_xgb_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.score(X_test , Y_test) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['status'].value_counts(normalize=True)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from pandas.plotting import scatter_matrix\n\n# attributes = list(data.columns.values)\n# scatter_matrix(data[attributes], figsize=(500.0, 500.0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import datasets\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\n\n\nclf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(n_estimators=50, random_state=1)\nclf3 = GaussianNB()\n\neclf = VotingClassifier(\n    estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n    voting='hard')\n\nfor clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n    scores = cross_val_score(clf, X, Y, scoring='accuracy', cv=5)\n    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eclf.fit(X_train,Y_train)\ny_pred=eclf.predict(X_test)\neclf.score(X_test , Y_test) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import datasets\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom itertools import product\nfrom sklearn.ensemble import VotingClassifier\n\n# Loading some example data\n\n# Training classifiers\nclf1 = DecisionTreeClassifier(max_depth=4)\nclf2 = KNeighborsClassifier(n_neighbors=7)\nclf3 = SVC(kernel='rbf', probability=True)\neclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)],\n                        voting='soft', weights=[4, 5, 3])\n\nclf1 = clf1.fit(X_train, Y_train)\nclf2 = clf2.fit(X_train, Y_train)\nclf3 = clf3.fit(X_train, Y_train)\neclf = eclf.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eclf.score(X_test, Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.datasets import make_blobs\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n\nclf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,\n    random_state=0)\nclf.fit(X_train,Y_train)\ny_pred=clf.predict(X_test)\nclf.score(X_test , Y_test) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier(n_estimators=10, max_depth=None,\n    min_samples_split=2, random_state=0)\nscores = cross_val_score(clf, X, Y, cv=5)\nscores.mean()\nclf.fit(X_train,Y_train)\ny_pred=clf.predict(X_test)\nclf.score(X_test , Y_test) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = ExtraTreesClassifier(n_estimators=100, max_depth=10,\n    min_samples_split=2, random_state=0)\nclf.fit(X_train,Y_train)\ny_pred=clf.predict(X_test)\nclf.score(X_test , Y_test) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Voting Ensemble for Classification\nimport pandas\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\n\nseed = 7\nkfold = model_selection.KFold(n_splits=10, random_state=seed)\n# create the sub models\nestimators = []\nmodel1 = LogisticRegression()\nestimators.append(('logistic', model1))\nmodel2 = DecisionTreeClassifier()\nestimators.append(('cart', model2))\nmodel3 = SVC()\nestimators.append(('svm', model3))\n# create the ensemble model\nensemble = VotingClassifier(estimators)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble.fit(X_train,Y_train)\ny_pred=ensemble.predict(X_test)\nensemble.score(X_test , Y_test) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stochastic Gradient Boosting Classification\nimport pandas\nfrom sklearn import model_selection\nfrom sklearn.ensemble import GradientBoostingClassifier\n# url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n# names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n# dataframe = pandas.read_csv(url, names=names)\n# array = dataframe.values\n# X = array[:,0:8]\n# Y = array[:,8]\n# seed = 7\nnum_trees = 100\nkfold = model_selection.KFold(n_splits=10, random_state=seed)\nmodel = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\nresults = model_selection.cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0).fit(X_train, Y_train)\nclf.score(X_test, Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier(n_estimators=10, max_depth=10,\n    min_samples_split=2, random_state=0)\nclf.fit(X_train,Y_train)\ny_pred=clf.predict(X_test)\nclf.score(X_test , Y_test) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport itertools\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nfrom sklearn import datasets\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nfrom mlxtend.classifier import StackingClassifier\n\nfrom sklearn.model_selection import cross_val_score, train_test_split\n\nfrom mlxtend.plotting import plot_learning_curves\nfrom mlxtend.plotting import plot_decision_regions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\nfrom scipy.stats import randint\n\nest = RandomForestClassifier(n_jobs=-1)\nrf_p_dist={'max_depth':[1,2,3,4,5,6,7,8,9,10],\n           'n_estimators':[1,2,3,4,5,10,11,12,13,100,200,300,400,500],\n              'max_features':[1,2,3,4,5],\n               'criterion':['gini','entropy'],\n               'bootstrap':[True,False],\n               'min_samples_leaf':[1,2,3,4,5]\n         \n              \n              }\n\nrs=GridSearchCV(estimator=est,param_grid=rf_p_dist)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = np.array(Y_train)\n\nx = np.array(X_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# iris = datasets.load_iris()\n# X, y = iris.data[:, 1:3], iris.target\n\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\n#create new a knn model\nknn = KNeighborsClassifier()\n#create a dictionary of all values we want to test for n_neighbors\nparams_knn = {'n_neighbors': np.arange(1, 2)}\n#use gridsearch to test all values for n_neighbors\nknn_gs = GridSearchCV(knn, params_knn, cv=5)\n\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# abcl = AdaBoostClassifier( n_estimators= 50)\n# clf2 = RandomForestClassifier(random_state=1)\n# clf3 = abcl\n\nclf2 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0)\nclf3 = RandomForestClassifier(n_estimators=10, max_depth=None,min_samples_split=2, random_state=0)\n\nlr = LogisticRegression()\nsclf = StackingClassifier(classifiers=[clf2, clf3], \n                          meta_classifier=lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label = ['KNN', 'Random Forest', 'Naive Bayes', 'Stacking Classifier']\nclf_list = [clf1, clf2, clf3, sclf]\n    \nfig = plt.figure(figsize=(10,8))\ngs = gridspec.GridSpec(2, 2)\ngrid = itertools.product([0,1],repeat=2)\n\nclf_cv_mean = []\nclf_cv_std = []\nfor clf, label, grd in zip(clf_list, label, grid):\n        \n    scores = cross_val_score(clf, x, y, cv=3, scoring='accuracy')\n    print(\"Accuracy: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label))\n    clf_cv_mean.append(scores.mean())\n    clf_cv_std.append(scores.std())\n        \n    clf.fit(x, y)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.score(X_test , Y_test) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot learning curves\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n    \nplt.figure()\nplot_learning_curves(X_train, Y_train, X_test, Y_test, sclf, print_model=False, style='ggplot')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import itertools\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nfrom sklearn import datasets\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import cross_val_score, train_test_split\n\nfrom mlxtend.plotting import plot_learning_curves\nfrom mlxtend.plotting import plot_decision_regions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    \n#XOR dataset\n#X = np.random.randn(200, 2)\n#y = np.array(map(int,np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)))\n    \nclf = DecisionTreeClassifier(criterion='entropy', max_depth=1)\n\nnum_est = [1, 2, 3, 10]\nlabel = ['AdaBoost (n_est=1)', 'AdaBoost (n_est=2)', 'AdaBoost (n_est=3)', 'AdaBoost (n_est=10)']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10, 8))\ngs = gridspec.GridSpec(2, 2)\ngrid = itertools.product([0,1],repeat=2)\n\nfor n_est, label, grd in zip(num_est, label, grid):     \n    boosting = AdaBoostClassifier(base_estimator=clf, n_estimators=n_est)   \n    boosting.fit(X, Y)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Ensemble Size\nnum_est = map(int, np.linspace(1,100,20))\nbg_clf_cv_mean = []\nbg_clf_cv_std = []\nfor n_est in num_est:\n    ada_clf = AdaBoostClassifier(base_estimator=clf, n_estimators=n_est)\n    scores = cross_val_score(ada_clf, X, Y, cv=3, scoring='accuracy')\n    bg_clf_cv_mean.append(scores.mean())\n    bg_clf_cv_std.append(scores.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n%matplotlib inline\n\nimport itertools\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nfrom sklearn import datasets\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.model_selection import cross_val_score, train_test_split\n\nfrom mlxtend.plotting import plot_learning_curves\nfrom mlxtend.plotting import plot_decision_regions\n\nnp.random.seed(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n    \nclf1 = DecisionTreeClassifier(criterion='entropy', max_depth=1)\nclf2 = KNeighborsClassifier(n_neighbors=1)    \n\nbagging1 = BaggingClassifier(base_estimator=clf1, n_estimators=10, max_samples=0.8, max_features=0.8)\nbagging2 = BaggingClassifier(base_estimator=clf2, n_estimators=10, max_samples=0.8, max_features=0.8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlabel = ['Decision Tree', 'K-NN', 'Bagging Tree', 'Bagging K-NN']\nclf_list = [clf1, clf2, bagging1, bagging2]\n\nfig = plt.figure(figsize=(10, 8))\ngs = gridspec.GridSpec(2, 2)\ngrid = itertools.product([0,1],repeat=2)\n\nfor clf, label, grd in zip(clf_list, label, grid):        \n    scores = cross_val_score(clf, X_train, Y_train, cv=3, scoring='accuracy')\n    print(\"Accuracy: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label))\n        \n    clf.fit(X_train, Y_train)\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.score(X_test,Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrfcl = RandomForestClassifier(n_estimators = 70)\nrfcl = rfcl.fit(X_train, Y_train)\ny_pred = rfcl.predict(X_test)\nrfcl.score(X_test , Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfcl.score(X_test , Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(X_train, Y_train)\ny_pred = dt.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test, y_pred)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nroc_auc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_depths = np.linspace(1, 32, 32, endpoint=True)\ntrain_results = []\ntest_results = []\nfor max_depth in max_depths:\n   dt = DecisionTreeClassifier(max_depth=max_depth)\n   dt.fit(X_train, Y_train)\n   train_pred = dt.predict(X_train)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_train, train_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   # Add auc score to previous train results\n   train_results.append(roc_auc)\n   y_pred = dt.predict(X_test)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test, y_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   # Add auc score to previous test results\n   test_results.append(roc_auc)\nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(max_depths, train_results, 'b', label='Train AUC')\nline2, = plt.plot(max_depths, test_results, 'r', label='Test AUC')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('Tree depth')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"min_samples_splits = np.linspace(0.1, 1.0, 10, endpoint=True)\ntrain_results = []\ntest_results = []\nfor min_samples_split in min_samples_splits:\n   dt = DecisionTreeClassifier(min_samples_split=min_samples_split)\n   dt.fit(X_train, Y_train)\n   train_pred = dt.predict(X_train)\n   false_positive_rate, true_positive_rate, thresholds =    roc_curve(Y_train, train_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   train_results.append(roc_auc)\n   y_pred = dt.predict(X_test)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test, y_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   test_results.append(roc_auc)\nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(min_samples_splits, train_results, 'b', label='Train AUC')\nline2, = plt.plot(min_samples_splits, test_results, 'r', label='Test AUC')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('min samples split')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"min_samples_leafs = np.linspace(0.1, 0.5, 5, endpoint=True)\ntrain_results = []\ntest_results = []\nfor min_samples_leaf in min_samples_leafs:\n   dt = DecisionTreeClassifier(min_samples_leaf=min_samples_leaf)\n   dt.fit(X_train, Y_train)\n   train_pred = dt.predict(X_train)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_train, train_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   train_results.append(roc_auc)\n   y_pred = dt.predict(X_test)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test, y_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   test_results.append(roc_auc)\nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(min_samples_leafs, train_results, 'b', label='Train AUC')\nline2, = plt.plot(min_samples_leafs, test_results, 'r', label='Test AUC')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('min samples leaf')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features = list(range(1,X_train.shape[1]))\ntrain_results = []\ntest_results = []\nfor max_feature in max_features:\n   dt = DecisionTreeClassifier(max_features=max_feature)\n   dt.fit(X_train, Y_train)\n   train_pred = dt.predict(X_train)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_train, train_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   train_results.append(roc_auc)\n   y_pred = dt.predict(X_test)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test, y_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   test_results.append(roc_auc)\nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(max_features, train_results, 'b', label='Train AUC')\nline2, = plt.plot(max_features, test_results, 'r', label='Test AUC')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('max features')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Count mis-classified one\ncount_misclassified = (Y_test != y_pred).sum()\nprint('Misclassified samples: {}'.format(count_misclassified))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB as gnb\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn import tree\nfrom os import system","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nDecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=20,\n            max_features=None, max_leaf_nodes=None,\n            min_impurity_split=1e-07, min_samples_leaf=6,\n            min_samples_split=1, min_weight_fraction_leaf=0.0,\n            presort=False, random_state=0, splitter='best')\nrfcl = RandomForestClassifier(n_estimators = 50)\nrfcl = rfcl.fit(X_train, Y_train)\ny_pred = rfcl.predict(X_test)\nrfcl.score(X_test , Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngbcl = GradientBoostingClassifier(n_estimators = 50, learning_rate = 0.05)\ngbcl = gbcl.fit(X_train,Y_train)\ny_pred = gbcl.predict(X_test)\ngbcl.score(X_test , Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting classifier to the Training set\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(max_depth=5, random_state=0)\nclassifier.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nabcl = AdaBoostClassifier( n_estimators= 50)\nabcl = abcl.fit(X_train,Y_train)\ny_pred = abcl.predict(X_test)\nabcl.score(X_test , Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Class 0', round(dataset['status'].value_counts()[0]/len(df) * 100,2), '% of the dataset-----', round(dataset['status'].value_counts()[0]))\nprint('Class 1', round(dataset['status'].value_counts()[1]/len(df) * 100,2), '% of the dataset-----', round(dataset['status'].value_counts()[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot('status',data=dataset)\nplt.title('New Distributin Dataset')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting classifier to the Training set\nfrom sklearn.svm import SVC\nclassifier = SVC(kernel = \"rbf\", random_state = 9)\nclassifier.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_test, y_pred)\ncm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.score(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}